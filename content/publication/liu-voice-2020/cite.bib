@article{liu_voice_2020,
 abstract = {Music source separation has been a popular topic in signal processing for decades, not only because of its technical difÔ¨Åculty, but also due to its importance to many commercial applications, such as automatic karoake and remixing. In this work, we propose a novel self-attention network to separate voice and accompaniment in music. First, a convolutional neural network (CNN) with densely-connected CNN blocks is built as our base network. We then insert self-attention subnets at different levels of the base CNN to make use of the long-term intra-dependency of music, i.e., repetition. Within self-attention subnets, repetitions of the same musical patterns inform reconstruction of other repetitions, for better source separation performance. Results show the proposed method leads to 19.5% relative improvement in vocals separation in terms of SDR. We compare our methods with state-of-the-art systems i.e. MMDenseNet [1] and MMDenseLSTM.[2].},
 author = {Liu, Yuzhou and Thoshkahna, Balaji and Milani, Ali and Kristjansson, Trausti},
 file = {Liu et al. - 2020 - Voice and accompaniment separation in music using .pdf:/Users/trausti/Zotero/storage/RBYMEJX5/Liu et al. - 2020 - Voice and accompaniment separation in music using .pdf:application/pdf},
 journal = {arXiv:2003.08954 [cs, eess, stat]},
 keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
 language = {en},
 month = {March},
 note = {arXiv: 2003.08954},
 title = {Voice and accompaniment separation in music using self-attention convolutional neural network},
 url = {http://arxiv.org/abs/2003.08954},
 urldate = {2021-05-19},
 year = {2020}
}

