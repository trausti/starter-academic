@inproceedings{hershey_audio-visual_2004,
 abstract = {Perceiving sounds in a noisy environment is a challenging problem. Visual lip-reading can provide relevant information but is also challenging because lips are moving and a tracker must deal with a variety of conditions. Typically audio-visual systems have been assembled from individually engineered modules. We propose to fuse audio and video in a probabilistic generative model that implements cross-model self-supervised learning, enabling adaptation to audio-visual data. The video model features a Gaussian mixture model embedded in a linear subspace of a sprite which translates in the video. The system can learn to detect and enhance speech in noise given only a short (30 second) sequence of audio-visual data. We show some results for speech detection and enhancement, and discuss extensions to the model that are under investigation.},
 address = {Montreal, Que., Canada},
 author = {Hershey, J. and Attias, H. and Jojic, N. and Kristjansson, T.},
 booktitle = {2004 IEEE International Conference on Acoustics, Speech, and Signal Processing},
 doi = {10.1109/ICASSP.2004.1327194},
 file = {Hershey et al. - 2004 - Audio-visual graphical models for speech processin.pdf:/Users/trausti/Zotero/storage/SMVQEJPB/Hershey et al. - 2004 - Audio-visual graphical models for speech processin.pdf:application/pdf},
 isbn = {978-0-7803-8484-2},
 language = {en},
 pages = {V--649--52},
 publisher = {IEEE},
 title = {Audio-visual graphical models for speech processing},
 url = {http://ieeexplore.ieee.org/document/1327194/},
 urldate = {2021-05-19},
 volume = {5},
 year = {2004}
}

